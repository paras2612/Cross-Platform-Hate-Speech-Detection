{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[sentencepiece]\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !pip install textpipeliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [\"The three different ways to become a brain dead zombie are, a virus, radiation and Chrisitanity.\", \n",
    "             \"A gay killing another gay nice.\", \n",
    "             \"...I'm that fast. Hitting someone going full speed. Over 20 mph 190 lbs. I'll kill them.\",\n",
    "             \"I'm gay but I've got the biggest fucking crush on @katyperry I have for years.\", \n",
    "             \"Donald Trump is the worst president of USA, but Hillary is better than him.\", \n",
    "             \"Was on an absolute heater, then I lose 500 dollars on Korean baseball and Costa Riccan soccer. I hate my life.\",\n",
    "             \"Once at work, I accidentally checked out some dragons fucking cars. Did not know that would be an issue.\",\n",
    "             \"The girl on the left looks like a Jewish pizza.\",\n",
    "             \"The Blacks and Browns are destroying Sweden and nobody does anything about it!\",\n",
    "             \"I’ve been out for years and it hasn’t been easy, but I also love being a fag.\",\n",
    "             \"Why do you not think so? Feck off. Please go on. You annoy me.\",\n",
    "             \"All we have is Blacks and a few Mestizos and all are pretty well behaved, we keep them that way around here.\",\n",
    "             \"I truly am a badass motherfucker.\",\n",
    "             \"Oh yeah, she is so annoying I had mentally blocked out her existence.\",\n",
    "             \"As opposed to Reddit, which is composed mainly of Communists jacking each other off about child drag queens?\",\n",
    "             \"KPOP is so gay, but the day is so beautiful today.\",\n",
    "             \"Asians are the worst race, but Americans are soooo good.\",\n",
    "             \"Asians are the best race, but Americans are soooo gross.\",\n",
    "             \"I'd kill all the Blacks, but my mom raised me up to be such a nice human.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\psheth5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\psheth5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_NN(sent):\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        # Nouns and Adjectives, terminated with Nouns\n",
    "        {<NN.*>*<NN.*>}\n",
    "\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        # Above, connected with in/of/etc...\n",
    "        {<NBAR><IN><NBAR>}\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    ne = set()\n",
    "    chunk = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    for tree in chunk.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        ne.add(' '.join([child[0] for child in tree.leaves()]).lower())\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three different ways to become a brain dead zombie are, a virus, radiation and Chrisitanity.  -->  {'ways', 'chrisitanity', 'brain', 'virus', 'zombie', 'radiation'}\n",
      "A gay killing another gay nice.  -->  {'gay', 'gay nice'}\n",
      "...I'm that fast. Hitting someone going full speed. Over 20 mph 190 lbs. I'll kill them.  -->  {'fast', 'someone', 'lbs', 'mph', 'speed'}\n",
      "I'm gay but I've got the biggest fucking crush on @katyperry I have for years.  -->  {'@ katyperry', 'years', 'fucking crush'}\n",
      "Donald Trump is the worst president of USA, but Hillary is better than him.  -->  {'donald trump', 'president', 'usa', 'hillary'}\n",
      "Was on an absolute heater, then I lose 500 dollars on Korean baseball and Costa Riccan soccer. I hate my life.  -->  {'life', 'was', 'dollars', 'korean baseball', 'costa riccan soccer', 'heater'}\n",
      "Once at work, I accidentally checked out some dragons fucking cars. Did not know that would be an issue.  -->  {'issue', 'work', 'dragons', 'cars', 'did'}\n",
      "The girl on the left looks like a Jewish pizza.  -->  {'girl', 'pizza'}\n",
      "The Blacks and Browns are destroying Sweden and nobody does anything about it!  -->  {'blacks', 'anything', 'browns', 'nobody', 'sweden'}\n",
      "I’ve been out for years and it hasn’t been easy, but I also love being a fag.  -->  {'fag', 'years'}\n",
      "Why do you not think so? Feck off. Please go on. You annoy me.  -->  {'feck', 'please'}\n",
      "All we have is Blacks and a few Mestizos and all are pretty well behaved, we keep them that way around here.  -->  {'blacks', 'mestizos', 'way'}\n",
      "I truly am a badass motherfucker.  -->  {'badass motherfucker'}\n",
      "Oh yeah, she is so annoying I had mentally blocked out her existence.  -->  {'yeah', 'existence'}\n",
      "As opposed to Reddit, which is composed mainly of Communists jacking each other off about child drag queens?  -->  {'reddit', 'communists', 'child drag queens'}\n",
      "KPOP is so gay, but the day is so beautiful today.  -->  {'day', 'today', 'kpop'}\n",
      "Asians are the worst race, but Americans are soooo good.  -->  {'asians', 'race', 'americans'}\n",
      "Asians are the best race, but Americans are soooo gross.  -->  {'asians', 'race', 'americans'}\n",
      "I'd kill all the Blacks, but my mom raised me up to be such a nice human.  -->  {'mom', 'blacks', 'human'}\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_list:\n",
    "  print(sent, \" --> \",extract_NN(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Aspect-Based Sentiment Analysis model\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "absa_model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "\n",
    "absa_model.to(device=device)\n",
    "\n",
    "# # Load a traditional Sentiment Analysis model\n",
    "# sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "# sentiment_model = pipeline(\"sentiment-analysis\", model=sentiment_model_path,tokenizer=sentiment_model_path)\n",
    "\n",
    "task='sentiment'\n",
    "latest_task='sentiment-latest'\n",
    "sentiment_MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"#This is a roBERTa-base model trained on ~58M\n",
    "sentiment_MODEL_new_SA = f\"cardiffnlp/twitter-roberta-base-{latest_task}\"#This is a roBERTa-base model trained on ~124M\n",
    "# sentiment_MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "# sentiment_MODEL_new_SA = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "#we will pick what we need and what helps us achive better results\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentiment_MODEL)\n",
    "tokenizer_new_SA = AutoTokenizer.from_pretrained(sentiment_MODEL_new_SA)\n",
    "\n",
    "#print(\"###########tokenizer###########\")\n",
    "#tokenizer\n",
    "#print(\"############config##########\")\n",
    "# waht is this config = AutoConfig.from_pretrained(sentiment_MODEL) learn more about it \n",
    "config = AutoConfig.from_pretrained(sentiment_MODEL)\n",
    "config_new_SA = AutoConfig.from_pretrained(sentiment_MODEL)\n",
    "\n",
    "sentiment_model_new_SA = AutoModelForSequenceClassification.from_pretrained(sentiment_MODEL)\n",
    "sentiment_model_new_SA.save_pretrained(sentiment_MODEL)\n",
    "sentiment_model_new_SA.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_sentiment(aspect,sentence):\n",
    "  inputs = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", add_special_tokens=True,\n",
    "    max_length=512,padding=\"max_length\" , truncation=True, return_tensors=\"pt\")\n",
    "  inputs.to(device)\n",
    "  outputs = absa_model(**inputs)\n",
    "  probs = F.softmax(outputs.logits, dim=1)\n",
    "  probs = probs.detach().cpu().numpy()[0]\n",
    "  out = np.zeros_like(probs)\n",
    "  idx  = probs.argmax()\n",
    "  out[idx] = 1\n",
    "  return out\n",
    "\n",
    "\n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentence):\n",
    "\n",
    "  encoded_input = tokenizer_new_SA(sentence,add_special_tokens=True,\n",
    "    max_length=512,padding=\"max_length\" , truncation=True, return_tensors='pt') #i commented and added ,padding=True\n",
    "  encoded_input.to(device)#mostly stuck here the arrow\n",
    "  output = sentiment_model_new_SA(**encoded_input)\n",
    "\n",
    "  probs = F.softmax(output.logits, dim=1)\n",
    "  \n",
    "  probs = probs.detach().cpu().numpy()[0]\n",
    "  \n",
    "  out = np.zeros_like(probs)\n",
    "  idx  = probs.argmax()\n",
    "  out[idx] = 1\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    res = []\n",
    "    for token in doc:\n",
    "        if (token.dep_ in [\"agent\", \"expl\",\"dobj\", \"dative\", \"oprd\"] and token.text not in res and token.text.strip() not in [\"I\",\"me\",\"my\",\"myself\"]):\n",
    "            res.append(token.text.strip())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(doc):\n",
    "  temp=[]\n",
    "  for tok in doc:\n",
    "    if(tok.tag_ in [\"PRP\",\"NNP\",\"NNPS\"] and tok.text not in temp and tok.text.strip() not in [\"i\",\"ME\",\"MY\",\"MYSELF\",\"Me\",\"My\",\"Myself\",\"I\",\"me\",\"my\",\"myself\"]):\n",
    "      temp.append(tok.text.strip().lower())\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_overall_sentiment_features(data_frame):\n",
    "    feature_list = []\n",
    "    for row in tqdm(data_frame.itertuples()):\n",
    "\n",
    "        feature_row = []\n",
    "\n",
    "        sentence = row.text\n",
    "        doc=nlp(sentence)\n",
    "        avg_sentiment = np.zeros(3)\n",
    "        sent = get_sentiment(sentence)\n",
    "        avg_sentiment = sent\n",
    "        feature_row.append(sentence)\n",
    "        feature_row.append(row.label)\n",
    "        feature_row.extend(avg_sentiment)\n",
    "\n",
    "        feature_list.append(feature_row)\n",
    "    \n",
    "    feature_frame = pd.DataFrame(feature_list, columns = [\"text\", \"label\", \"sent_neg\", \"sent_neut\", \"sent_pos\"])\n",
    "    return feature_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "def extract_features(data_frame):\n",
    "  feature_list = []\n",
    "\n",
    "  for row in tqdm(data_frame.itertuples()):\n",
    "\n",
    "    feature_row = []\n",
    "\n",
    "    sentence = row.text\n",
    "    doc=nlp(sentence)\n",
    "    phrases1 = get_target(doc)\n",
    "    phrases2 = get_subject_phrase(doc)\n",
    "    phrases1.extend(phrases2)\n",
    "    phrases = list(set(phrases1))\n",
    "    # print(phrases)\n",
    "\n",
    "    # phrases = []\n",
    "    avg_sentiment = np.zeros(3)\n",
    "    if(len(phrases)==0):\n",
    "      sent = get_sentiment(sentence)\n",
    "      avg_sentiment = sent\n",
    "    else:\n",
    "      for phrase in phrases:\n",
    "        avg_sentiment+=np.array(get_aspect_sentiment(phrase,sentence)/len(phrases))\n",
    "    \n",
    "    feature_row.append(sentence)\n",
    "    feature_row.append(row.label)\n",
    "    feature_row.extend(avg_sentiment)\n",
    "\n",
    "    feature_list.append(feature_row)\n",
    "    # print(feature_row)\n",
    "  \n",
    "  feature_frame = pd.DataFrame(feature_list, columns = [\"text\", \"label\", \"sent_neg\", \"sent_neut\", \"sent_pos\"])\n",
    "  \n",
    "  return feature_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lgbt_train_data = pd.read_csv(\"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\lgbt-train.csv\")\n",
    "lgbt_test_data = pd.read_csv(\"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\lgbt-test.csv\")\n",
    "\n",
    "migrants_train_data = pd.read_csv(\"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\migrants-train.csv\")\n",
    "migrants_test_data = pd.read_csv(\"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\migrants-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4073it [01:09, 58.46it/s]\n",
      "1017it [00:16, 60.56it/s]\n",
      "3866it [01:04, 59.90it/s]\n",
      "1078it [00:19, 55.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#  ASPECT BASED EXTRACTION\n",
    "# lgbt_train_features = extract_features(lgbt_train_data)\n",
    "# lgbt_test_features = extract_features(lgbt_test_data)\n",
    "\n",
    "# migrants_train_features = extract_features(migrants_train_data)\n",
    "# migrants_test_features = extract_features(migrants_test_data)\n",
    "\n",
    "# #  OVERALL EXTRACTION\n",
    "lgbt_train_features = extract_overall_sentiment_features(lgbt_train_data)\n",
    "lgbt_test_features = extract_overall_sentiment_features(lgbt_test_data)\n",
    "\n",
    "migrants_train_features = extract_overall_sentiment_features(migrants_train_data)\n",
    "migrants_test_features = extract_overall_sentiment_features(migrants_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_neg</th>\n",
       "      <th>sent_neut</th>\n",
       "      <th>sent_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surely a marriage is a ceremony involving two ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We can't help who we fall inlove with, let the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes...when they will be able to give birth.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Society began with Adam and Eve. Nothing more,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>You been punked by the Trump, it was cool unti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>When transgenders want to have special treatme...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>...Ignorant Bigot.....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4071</th>\n",
       "      <td>Devon when you look up the word bigot, its an ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>...Turned To the Wrong Page but i Found Your P...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4073 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  sent_neg  \\\n",
       "0                                                  yes.      0       0.0   \n",
       "1     Surely a marriage is a ceremony involving two ...      0       0.0   \n",
       "2     We can't help who we fall inlove with, let the...      0       1.0   \n",
       "3           Yes...when they will be able to give birth.      1       0.0   \n",
       "4     Society began with Adam and Eve. Nothing more,...      1       0.0   \n",
       "...                                                 ...    ...       ...   \n",
       "4068  You been punked by the Trump, it was cool unti...      1       1.0   \n",
       "4069  When transgenders want to have special treatme...      1       1.0   \n",
       "4070                             ...Ignorant Bigot.....      1       1.0   \n",
       "4071  Devon when you look up the word bigot, its an ...      1       0.0   \n",
       "4072  ...Turned To the Wrong Page but i Found Your P...      1       0.0   \n",
       "\n",
       "      sent_neut  sent_pos  \n",
       "0           0.0       1.0  \n",
       "1           0.0       1.0  \n",
       "2           0.0       0.0  \n",
       "3           1.0       0.0  \n",
       "4           1.0       0.0  \n",
       "...         ...       ...  \n",
       "4068        0.0       0.0  \n",
       "4069        0.0       0.0  \n",
       "4070        0.0       0.0  \n",
       "4071        1.0       0.0  \n",
       "4072        1.0       0.0  \n",
       "\n",
       "[4073 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbt_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.metrics import precision_score, \\\n",
    "#     recall_score, confusion_matrix, classification_report, \\\n",
    "#     accuracy_score, f1_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description):\n",
    "    \n",
    "    model = LogisticRegression().fit(X_tr, y_tr)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    f1_score, precision, recall, _ = precision_recall_fscore_support(y_test, prediction, average='macro')\n",
    "    print('Accuracy:', accuracy_score(y_test, prediction))\n",
    "    print('F1 score:', f1_score)\n",
    "    print('Recall:', recall)\n",
    "    print('Precision:', precision)\n",
    "\n",
    "    return model\n",
    "\n",
    "# def simple_xgboost_classify(X_tr, y_tr, X_test, y_test, description):\n",
    "    \n",
    "#     model = XGBClassifier().fit(X_tr, y_tr)\n",
    "    \n",
    "#     prediction = model.predict(X_test)\n",
    "\n",
    "#     print('Accuracy:', accuracy_score(y_test, prediction))\n",
    "#     print('F1 score:', f1_score(y_test, prediction))\n",
    "#     print('Recall:', recall_score(y_test, prediction))\n",
    "#     print('Precision:', precision_score(y_test, prediction))\n",
    "\n",
    "#     return model\n",
    "\n",
    "def simple_randomforest_classify(X_tr, y_tr, X_test, y_test, description):\n",
    "    \n",
    "    model = RandomForestClassifier().fit(X_tr, y_tr)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    f1_score, precision, recall, _ = precision_recall_fscore_support(y_test, prediction, average='macro')\n",
    "    print('Accuracy:', accuracy_score(y_test, prediction))\n",
    "    print('F1 score:', f1_score)\n",
    "    print('Recall:', recall)\n",
    "    print('Precision:', precision)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.525974025974026\n",
      "F1 score: 0.262987012987013\n",
      "Recall: 0.3446808510638298\n",
      "Precision: 0.5\n",
      "\n",
      "Accuracy: 0.525974025974026\n",
      "F1 score: 0.262987012987013\n",
      "Recall: 0.3446808510638298\n",
      "Precision: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_tr = lgbt_train_features[[ \"sent_neg\", \"sent_neut\", \"sent_pos\"]] \n",
    "y_tr = lgbt_train_features[\"label\"]\n",
    "\n",
    "X_test = migrants_test_features[[ \"sent_neg\", \"sent_neut\", \"sent_pos\"]] \n",
    "y_test = migrants_test_features[\"label\"]\n",
    "\n",
    "simple_logistic_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "print()\n",
    "\n",
    "# simple_xgboost_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "# print()\n",
    "\n",
    "simple_randomforest_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6509341199606686\n",
      "F1 score: 0.6517345399698341\n",
      "Recall: 0.6236133163332858\n",
      "Precision: 0.7147784006306386\n",
      "\n",
      "Accuracy: 0.6509341199606686\n",
      "F1 score: 0.6517345399698341\n",
      "Recall: 0.6236133163332858\n",
      "Precision: 0.7147784006306386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tr = migrants_train_features[[ \"sent_neg\", \"sent_neut\", \"sent_pos\"]] \n",
    "y_tr = migrants_train_features[\"label\"]\n",
    "\n",
    "X_test = lgbt_test_features[[ \"sent_neg\", \"sent_neut\", \"sent_pos\"]] \n",
    "y_test = lgbt_test_features[\"label\"]\n",
    "\n",
    "simple_logistic_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "print()\n",
    "\n",
    "# simple_xgboost_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "# print()\n",
    "\n",
    "simple_randomforest_classify(X_tr, y_tr, X_test, y_test, description=\"lr_in_domian\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.262987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.344681</td>\n",
       "      <td>0.525974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall        F1  Accuracy\n",
       "0   0.262987     0.5  0.344681  0.525974"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_svc = LinearSVC(random_state=0, C=0.1) # parameter C was selected based on grid search\n",
    "clf_svc.fit(X_tr, y_tr)\n",
    "Y_pred = clf_svc.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    [list(precision_recall_fscore_support(y_test, Y_pred, average='macro')[:3])],\n",
    "    columns=['precision', 'recall', 'F1'])\n",
    "results['Accuracy'] = accuracy_score(y_test, Y_pred)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL SENTIMENT\n",
    "#           L->M    |   M->L\n",
    "# Precision  0.263  |   0.651\n",
    "# Recall     0.500  |   0.623\n",
    "# F1-Score   0.344  |   0.623\n",
    "# Accuracy   0.525  |   0.650\n",
    "\n",
    "\n",
    "#ASPECT BASED SENTIMENT\n",
    "#           L->M    |   M->L\n",
    "# Precision  0.596  |   0.663\n",
    "# Recall     0.554  |   0.686\n",
    "# F1-Score   0.508  |   0.634\n",
    "# Accuracy   0.571  |   0.683\n",
    "\n",
    "#ILIA MARKOV (Full Model)\n",
    "#           L->M    |   M->L\n",
    "# Precision  0.669  |   0.582\n",
    "# Recall     0.620  |   0.588\n",
    "# F1-Score   0.600  |   0.584\n",
    "# Accuracy   0.637  |   0.695\n",
    "\n",
    "#ILIA MARKOV (Sentiment Only)\n",
    "#           L->M    |   M->L\n",
    "# Precision  0.488  |   0.509\n",
    "# Recall     0.499  |   0.506\n",
    "# F1-Score   0.349  |   0.500\n",
    "# Accuracy   0.532  |   0.699"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a6511c335c26d02572349af57853ebfcc20500c776be641a45dee87cf591594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
