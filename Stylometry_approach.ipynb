{"cells":[{"cell_type":"markdown","metadata":{"id":"QkBEmLTzdn5U"},"source":["Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1013,"status":"ok","timestamp":1629293510303,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"qvCFY4fOdSF6","outputId":"afca7f08-2b7c-4d6d-e567-2a7f4822ee84"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\psheth5\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import nltk, csv, collections\n","nltk.download('punkt')\n","from sklearn.feature_extraction.text import CountVectorizer \n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n","from scipy.sparse import hstack"]},{"cell_type":"markdown","metadata":{"id":"BKMaawW-dr_E"},"source":["Load data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1629293510303,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"9EAdNMkadygu","outputId":"1f24f630-9bf9-49d4-dce5-9f4eaf2c142d"},"outputs":[],"source":["# # load data from Google drive\n","# from google.colab import drive\n","# drive.mount('/content/drive') # mount drive\n","\n","# path to the datasets on Google drive (training and test subsets of the LGBT and migrants datasets)\n","lgbt_test = \"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\lgbt-test.csv\"\n","lgbt_train = \"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\lgbt-train.csv\"\n","migrants_test = \"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\migrants-train.csv\"\n","migrants_train = \"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\migrants-test.csv\"\n","\n","train = pd.read_csv(migrants_train)\n","test = pd.read_csv(lgbt_test)\n","# concatenate the training and test subsets of the LGBT and migrants datasets\n","# train = pd.concat([pd.read_csv(lgbt_train, sep=','), pd.read_csv(migrants_train, sep=',')]).reset_index(drop=True)\n","# test = pd.concat([pd.read_csv(lgbt_test, sep=','), pd.read_csv(migrants_test, sep=',')]).reset_index(drop=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1629293510304,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"mXf_lxayd6sQ","outputId":"99194837-6fbb-46b9-a908-fb32bc58e490"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>exact_label</th>\n","      <th>label</th>\n","      <th>tokens</th>\n","      <th>lemmas</th>\n","      <th>upos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10153273111682217</td>\n","      <td>Why should we? It's the biggest humanitarian c...</td>\n","      <td>Acceptable speech</td>\n","      <td>0</td>\n","      <td>Why should we ? It 's the biggest humanitarian...</td>\n","      <td>why should we ? it be the biggest humanitarian...</td>\n","      <td>ADV AUX PRON PUNCT PRON AUX DET ADJ ADJ NOUN A...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10153273119732217</td>\n","      <td>these refugees adult males are cowards for not...</td>\n","      <td>Background offensive</td>\n","      <td>1</td>\n","      <td>these refugees adult males are cowards for not...</td>\n","      <td>these refuge adult male be cowards for not def...</td>\n","      <td>DET NOUN ADJ NOUN AUX NOUN SCONJ ADV VERB PRON...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  id                                               text  \\\n","0  10153273111682217  Why should we? It's the biggest humanitarian c...   \n","1  10153273119732217  these refugees adult males are cowards for not...   \n","\n","            exact_label  label  \\\n","0     Acceptable speech      0   \n","1  Background offensive      1   \n","\n","                                              tokens  \\\n","0  Why should we ? It 's the biggest humanitarian...   \n","1  these refugees adult males are cowards for not...   \n","\n","                                              lemmas  \\\n","0  why should we ? it be the biggest humanitarian...   \n","1  these refuge adult male be cowards for not def...   \n","\n","                                                upos  \n","0  ADV AUX PRON PUNCT PRON AUX DET ADJ ADJ NOUN A...  \n","1  DET NOUN ADJ NOUN AUX NOUN SCONJ ADV VERB PRON...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# display the data (note that in the preprocessing step, the texts were tokenized, lemmatized, and universal POS tags were extracted into the corresponding columns)\n","train.head(2)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1629293510503,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"UIuePrnGXXL3","outputId":"a3be98cf-3847-4969-9440-aaab961be014"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>exact_label</th>\n","      <th>label</th>\n","      <th>tokens</th>\n","      <th>lemmas</th>\n","      <th>upos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10155108649247217</td>\n","      <td>This is the problem with having a referendum o...</td>\n","      <td>Acceptable speech</td>\n","      <td>0</td>\n","      <td>This is the problem with having a referendum o...</td>\n","      <td>this be the problem with have a referendum on ...</td>\n","      <td>PRON AUX DET NOUN SCONJ VERB DET NOUN ADP PRON...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10155108659877217</td>\n","      <td>I live in the UK, so luckily our government ha...</td>\n","      <td>Acceptable speech</td>\n","      <td>0</td>\n","      <td>I live in the UK , so luckily our government h...</td>\n","      <td>I live in the UK , so luckily we government ha...</td>\n","      <td>PRON VERB ADP DET PROPN PUNCT ADV ADV PRON NOU...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  id                                               text  \\\n","0  10155108649247217  This is the problem with having a referendum o...   \n","1  10155108659877217  I live in the UK, so luckily our government ha...   \n","\n","         exact_label  label  \\\n","0  Acceptable speech      0   \n","1  Acceptable speech      0   \n","\n","                                              tokens  \\\n","0  This is the problem with having a referendum o...   \n","1  I live in the UK , so luckily our government h...   \n","\n","                                              lemmas  \\\n","0  this be the problem with have a referendum on ...   \n","1  I live in the UK , so luckily we government ha...   \n","\n","                                                upos  \n","0  PRON AUX DET NOUN SCONJ VERB DET NOUN ADP PRON...  \n","1  PRON VERB ADP DET PROPN PUNCT ADV ADV PRON NOU...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["test.head(2)"]},{"cell_type":"markdown","metadata":{"id":"yOcfsFhZeOQW"},"source":["Load NRC emotion lexicon"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1629293510504,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"X8FnzqKTeMYD","outputId":"1e1198d0-0eb3-4db9-bcd7-4473486cae19"},"outputs":[{"data":{"text/plain":["[('smut', ['negative']),\n"," ('expletive', ['negative']),\n"," ('greeting', ['surprise'])]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# load the NRC emotion lexicon into a dictionary with emotion words and corresponding associations\n","# lexicon = '/content/drive/My Drive/Stylometry-approach-HS/nrc-lexicon-en.txt' # path to the NRC emotion lexicon on Google drive\n","lexicon = \"C:\\\\Users\\\\psheth5\\\\Downloads\\\\Stylometric-emotion-approach (1)\\\\Stylometric-emotion-approach\\\\nrc-lexicon-en.txt\"\n","emotions = {}\n","for line in open(lexicon).read().split('\\n'):\t\n","\temotion_word = line.split('\\t')[0]\n","\temotion = line.split('\\t')[1]\n","\tassociation = line.split('\\t')[2]\n","\tif association == \"1\":\n","\t\tif emotion_word in emotions and emotion_word in ['positive','negative','neutral']:\n","\t\t\temotions[emotion_word].append(emotion)\n","\t\telse:\n","\t\t\temotions[emotion_word] = [emotion] \n","\n","list(emotions.items())[:3] # print first 3 entries"]},{"cell_type":"markdown","metadata":{"id":"b94QXueke3fj"},"source":["Features"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4870,"status":"ok","timestamp":1629293515370,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"cgoBik0_ewkc"},"outputs":[],"source":["# extract features as described in the paper:\n","# - pos_fw_emo = representation of the text through POS tags, function words, and emotion words (from this representation n-grams (n=1-3) are built, see vectorize below)\n","# - count = number of emotion words in a text\n","# - emotion_associations = emotion associations from the NRC emotion lexicon\n","\n","fw_list = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ'] # POS tags that correspond to function words\n","\n","def get_feats_en(upos, lemmas):\t\n","  pos_fw_emo = []\n","  count = 0\n","  emotion_associations = []\n","  for i, lemma in enumerate(lemmas.split()):\t\t\n","    if lemma.lower() in emotions:\n","      pos_fw_emo.append(lemma)\n","      count += 1\n","      temp = emotions[lemma.lower()]\n","      temp1 = []\n","      if(\"positive\" in temp):\n","        temp1.append(\"positive\")\n","      if(\"negattive\" in temp):\n","        temp1.append(\"negative\")\n","      if(\"neutral\" in temp):\n","        temp1.append(\"neutral\")\n","      emotion_associations.append(temp1)     \n","    else:\n","      if upos.split()[i] in fw_list:\n","        pos_fw_emo.append(lemma)\n","      else:\n","        pos_fw_emo.append(upos.split()[i])\n","  emotion_associations = [emo for sublist in emotion_associations for emo in sublist]\n","  return pd.Series([' '.join(pos_fw_emo), count, ' '.join(emotion_associations)])\n","\n","train[['pos_fw_emo', 'count', 'emotion_associations']] = train.apply(lambda x: get_feats_en(x['upos'], x['lemmas']), axis=1) \n","test[['pos_fw_emo', 'count', 'emotion_associations']] = test.apply(lambda x: get_feats_en(x['upos'], x['lemmas']), axis=1) "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1629293515371,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"jNkrzWavgyXU","outputId":"d45f5e00-0121-4328-e6d7-af1e500d736d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>exact_label</th>\n","      <th>label</th>\n","      <th>tokens</th>\n","      <th>lemmas</th>\n","      <th>upos</th>\n","      <th>pos_fw_emo</th>\n","      <th>count</th>\n","      <th>emotion_associations</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10153273111682217</td>\n","      <td>Why should we? It's the biggest humanitarian c...</td>\n","      <td>Acceptable speech</td>\n","      <td>0</td>\n","      <td>Why should we ? It 's the biggest humanitarian...</td>\n","      <td>why should we ? it be the biggest humanitarian...</td>\n","      <td>ADV AUX PRON PUNCT PRON AUX DET ADJ ADJ NOUN A...</td>\n","      <td>ADV should we PUNCT it be the ADJ humanitarian...</td>\n","      <td>2</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10153273119732217</td>\n","      <td>these refugees adult males are cowards for not...</td>\n","      <td>Background offensive</td>\n","      <td>1</td>\n","      <td>these refugees adult males are cowards for not...</td>\n","      <td>these refuge adult male be cowards for not def...</td>\n","      <td>DET NOUN ADJ NOUN AUX NOUN SCONJ ADV VERB PRON...</td>\n","      <td>these NOUN ADJ NOUN be NOUN for ADV defend the...</td>\n","      <td>1</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  id                                               text  \\\n","0  10153273111682217  Why should we? It's the biggest humanitarian c...   \n","1  10153273119732217  these refugees adult males are cowards for not...   \n","\n","            exact_label  label  \\\n","0     Acceptable speech      0   \n","1  Background offensive      1   \n","\n","                                              tokens  \\\n","0  Why should we ? It 's the biggest humanitarian...   \n","1  these refugees adult males are cowards for not...   \n","\n","                                              lemmas  \\\n","0  why should we ? it be the biggest humanitarian...   \n","1  these refuge adult male be cowards for not def...   \n","\n","                                                upos  \\\n","0  ADV AUX PRON PUNCT PRON AUX DET ADJ ADJ NOUN A...   \n","1  DET NOUN ADJ NOUN AUX NOUN SCONJ ADV VERB PRON...   \n","\n","                                          pos_fw_emo  count  \\\n","0  ADV should we PUNCT it be the ADJ humanitarian...      2   \n","1  these NOUN ADJ NOUN be NOUN for ADV defend the...      1   \n","\n","  emotion_associations  \n","0                       \n","1             positive  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train.head(2)"]},{"cell_type":"markdown","metadata":{"id":"5Sbc7-JCJx_l"},"source":["Vectorize"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1629293516859,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"X6NSmvqzJ2LJ"},"outputs":[],"source":["vectorizer1 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 3)) # to build n-grams (n=1-3) from the pos_fw_emo representation\n","vectorizer2 = CountVectorizer(tokenizer=lambda x: x.split(), analyzer='word', ngram_range=(1, 1)) # unigrams of emotion associations\n","\n","# combine the features\n","X_train = vectorizer2.fit_transform(train.emotion_associations)\n","X_test =  vectorizer2.transform(test.emotion_associations)\n","\n","Y_train = train.label.values\n","Y_test = test.label.values"]},{"cell_type":"markdown","metadata":{"id":"q4b6gcD9KGQz"},"source":["Classify"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2243,"status":"ok","timestamp":1629293519101,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"7GTKZIe8KDgZ","outputId":"9f401dcf-b7e0-4697-fb93-98baf1abcd07"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\psheth5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["clf_svc = LinearSVC(random_state=0, C=0.1) # parameter C was selected based on grid search\n","clf_svc.fit(X_train, Y_train)\n","Y_pred = clf_svc.predict(X_test)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# from sklearn.linear_model import LogisticRegression\n","\n","# model = LogisticRegression().fit(X_train, Y_train)\n","# Y_pred = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"3jNdGssyKqNc"},"source":["Results"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1629293519102,"user":{"displayName":"Ilia Markov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKKVHQAMPDKer006hWNwA1h_Tl0RFH-J_2tc01=s64","userId":"09447936546757127632"},"user_tz":-120},"id":"DqPB1yvcNH2t","outputId":"6cbe5acf-f940-4aa7-a80d-771b01a11ca7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.509105</td>\n","      <td>0.50623</td>\n","      <td>0.500805</td>\n","      <td>0.699115</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   precision   recall        F1  Accuracy\n","0   0.509105  0.50623  0.500805  0.699115"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["results = pd.DataFrame(\n","    [list(precision_recall_fscore_support(test.label, Y_pred, average='macro')[:3])],\n","    columns=['precision', 'recall', 'F1'])\n","results['Accuracy'] = accuracy_score(test.label, Y_pred)\n","results"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# LGBT to MIGRANT\n","precision = 0.67\n","recall = 0.62\n","F1 = 0.60\n","\n","# MIGRANT to LGBT\n","precision = 0.58\n","recall = 0.58\n","F1 = 0.58"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN5Fza4gFFyUWN5APNx5IM6","name":"Stylometry_approach.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"8a6511c335c26d02572349af57853ebfcc20500c776be641a45dee87cf591594"}}},"nbformat":4,"nbformat_minor":0}
